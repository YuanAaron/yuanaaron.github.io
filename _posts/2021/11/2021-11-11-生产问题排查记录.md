---
layout: post 
author: oshacker
title: 生产问题排查记录
category: prod
tags: [prod]
excerpt: 第一篇
---

## 磁盘满了

> 现象：系统卡顿（比如登陆响应了半天才进去，查询也很卡），但是系统又没挂掉（进程还在）

```shell
df -h # df（disk free）-h(human-readable)
```

一般查看/dev/vda1的使用率，如果发现磁盘使用达到99.x%，即磁盘满了。而系统运行时肯定会写日志，这样就无法再写入或写入很慢。

接下来，定位具体是哪个目录/文件占用了大量的磁盘空间。

```shell
du -h --max-depth=2 /usr # /usr表示从哪个目录查找
du --block-size=MB --max-depth=2 /usr | sort -n -r -k1 | head -n5

其中，
--block-size：统一单位方便sort
-n：按数字方式排序
-r：倒序
-k1：k表示按列排序（默认按行排序），后面的数字表示按哪一列排序，比如这里的按第一列排序
head -n5：取前5条
```

如果最终发现是日志占用了大量的磁盘空间，临时的方案是手动删除过期的日志。还可以在springboot的logback中定义滚动日志，设置保存多久的日志，比如只保留7天的日志。

## 用户无法连接

> 现象：Error: open too many files.

一般排查Linux服务器的最大连接数（默认为1024），如果过小，可以将其修改为65535。

```shell
ulimit -n
ulimit -n 65535 # 临时的修改方法
# 永久的修改方法
vim /etc/security/limits.conf
* soft nofile 65535
* hard nofile 65535
```

即使修改成65535，也有可能会报连接满了，原因可能是很多连接处在TIME_WAIT 或CLOSE_WAIT状态。

+ TIME_WAIT

主动关闭方，可调整等待时间。

场景：主动关闭方是爬虫服务器时，它会爬取很多服务器的数据（建立大量连接），如果大量连接关闭后进入TIME_WAIT状态（等待2MSL时间），即连接仍然占用，这时就无法创建新连接。

+ CLOSE_WAIT

场景：在游戏服务器中，很多客户端（主动关闭方）主动断开连接，导致服务器（被动关闭方）很多连接处在CLOSE_WAIT，但由于某些原因，服务端没有给客户端发送ACK或SYN，这样客户端也就不会给服务端发送ACK，服务端连接也就无法关闭。

```shell
netstat -n | awk '/^tcp/ {++S[$NF]} END {for(a in S) print a, S[a]}' # 统计各个TCP状态的连接数
```

CLOSE_WAIT是由TCP的keep_alive设置的（默认是2h，针对整个服务器的所有连接），不建议修改。建议修改服务端代码，自定义发送心跳（TCP的keep_alive相当于一个心跳，但这个心跳时间太长了），如果发现主动方已经关闭，那么自己主动将CLOSE_WAIT状态变为CLOSED状态。

## CPU飙高

现象：系统卡顿，没挂，能用，就是慢。

正如前面所述，首先排查磁盘是否满了。如果磁盘没有满，可以查看CPU使用情况。

```shell
top # 查看进程状态，即每个进程占用的CPU、内存等情况
top -H -p <pid> # 其中pid为进程id，具体哪个线程占用的CPU、内存等比较高
jstack <pid> | grep <tid> -A 10 # 查看某个线程的线程栈情况，其中tid为线程id的十六进制形式,-A表示显示后面的多少行

# openjdk没有jstack
yum search jdk | grep 8
yum install -y java-1.8.0-openjdk-devel.x86_64 
```

实战案例：

1. 执行top命令，然后shift+p按照CPU占用排序，发现pid为20911的进程CPU占用很高。

![image-20211112172839055](https://cdn.jsdelivr.net/gh/YuanAaron/BlogImage/2021/image-20211112172839055.png)

2. 执行top -H -p 20911，发现id为20927的线程CPU占用很高

![image-20211112173445140](https://cdn.jsdelivr.net/gh/YuanAaron/BlogImage/2021/image-20211112173445140.png)

2. 执行jstack 20911 | grep 51bf -A 10命令，其线程栈情况如下：

![image-20211112173903920](https://cdn.jsdelivr.net/gh/YuanAaron/BlogImage/2021/image-20211112173903920.png)

2. 查看BusinessController类的第34行，代码如下：

![image-20211112174302006](https://cdn.jsdelivr.net/gh/YuanAaron/BlogImage/2021/image-20211112174302006.png)

经过排查发现，是业务代码线程导致的CPU飙高，此外，GC线程等也可能导致CPU飙高，关于这方面的排查后面再介绍。

## 堆内存溢出

